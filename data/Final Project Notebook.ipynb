{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration and Visualization - Taxi Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from mpl_toolkits.basemap import Basemap\n",
    "\n",
    "import os\n",
    "\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib as mpl\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.notebook_repr_html', True)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\", {'axes.grid' : False})\n",
    "sns.set_context(\"poster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant Links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Data (Yellow Cab, 2013): http://www.andresmh.com/nyctaxitrips/\n",
    "\n",
    "* Data (Yellow cab, June 2015): http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml\n",
    "\n",
    "* *Incredible* visualization: http://nyctaxi.herokuapp.com/\n",
    "\n",
    "**Privacy Concerns**\n",
    "\n",
    "* Privacy concerns with the dataset: http://research.neustar.biz/2014/09/15/riding-with-the-stars-passenger-privacy-in-the-nyc-taxicab-dataset/\n",
    "\n",
    "* Locating Muslim cab drivers: http://mashable.com/2015/01/28/redditor-muslim-cab-drivers/#PJCrpyV6tPqY\n",
    "\n",
    "**Uber Disruption**\n",
    "\n",
    "* October 13, 2015: http://fivethirtyeight.com/features/uber-is-taking-millions-of-manhattan-rides-away-from-taxis/\n",
    "\n",
    "* August 10, 2015: http://fivethirtyeight.com/features/uber-is-serving-new-yorks-outer-boroughs-more-than-taxis-are/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brainstorming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Uber data, we could look at change in pickups between taxis and Uber. 538 already did this (see 10/13/15 article).\n",
    "\n",
    "So taxi cab drivers *are* full-time employees. Cite that. However, Uber drivers are treated as independent contractors. This changes their compensation package, tax structure, and most importantly benefits (i.e. no health care)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, we use publically available NYC Yellow Taxi Cab data from June 2013. The entire dataset is too large to work with, so we obtained a subsample of the dataset. The subsample comes as two files: \"fare\" data (1.3 GB) and \"trip\" data (2.3 GB). We import both files and merge them to obtain complete information about each ride. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df2015 = pd.read_csv('raw_data/yellow_tripdata_2015-06.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#df2013trip = pd.read_csv('raw_data/sample_trip.csv')\n",
    "#df2013fare = pd.read_csv('raw_data/sample_fare.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Because the dataset is too large, we only pull information for the first 1 million trips\n",
    "ssize = 1000000\n",
    "df2013trip = pd.read_csv('raw_data/trip_data_1.csv', nrows=ssize)\n",
    "df2013fare = pd.read_csv('raw_data/trip_fare_1.csv', nrows=ssize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataset containing trip information\n",
    "df2013trip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dataset containing fare information\n",
    "df2013fare.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we merge and clean the datasets. The column names are slightly different. And information about the trip and fare are split into separate data chunks. They can be merged into a single dataframe based on columns like medallion - a hash value that represents a unique taxi driver.\n",
    "\n",
    "See the difference in column names?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df2013trip.columns\n",
    "print \n",
    "print df2013fare.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#rename the columns to elminate the leading white space \n",
    "df2013fare.rename(columns={\n",
    "        ' hack_license' : 'hack_license',\n",
    "        ' vendor_id' : 'vendor_id',\n",
    "        ' pickup_datetime' : 'pickup_datetime',\n",
    "        ' payment_type' : 'payment_type',\n",
    "        ' fare_amount' : 'fare_amount',\n",
    "        ' surcharge' : 'surcharge',\n",
    "        ' mta_tax' : 'mta_tax',\n",
    "        ' tip_amount' : 'tip_amount',\n",
    "        ' tolls_amount' : 'tolls_amount',\n",
    "        ' total_amount' : 'total_amount'\n",
    "    }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we merge our two data sets containing trip and fare information for the same cab trips into a single dataframe called `merged2013df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#merge the two datasets\n",
    "merged2013dffull = pd.merge(df2013trip, df2013fare, on=['medallion', 'hack_license', 'vendor_id', 'pickup_datetime'], how='inner')\n",
    "merged2013dffull.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clean our tip data. The first thing we do is add a column called `tip_amount_normalized` that calculates the tip as a percentage of the fare amount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2013dffull['tip_amount_normalized'] = merged2013dffull.tip_amount/merged2013dffull.fare_amount\n",
    "merged2013dffull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we eliminate the row with missing or inaccurate latitude or longitude information. Some entries have latitudes and longitudes that don't make sense, such as the middle of the ocean. These errors will skew our analysis so we remove those entries as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2013dffull = merged2013dffull[merged2013dffull.pickup_latitude.between(40.65,40.85)]\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.pickup_longitude.between(-74.025,-73.85)]\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.dropoff_latitude.between(40.65,40.85)]\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.dropoff_longitude.between(-74.025,-73.85)]\n",
    "merged2013dffull = merged2013dffull.reset_index(drop=True)\n",
    "merged2013dffull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Where are people going?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to look at where people are going. Let's create index variables to represent the rate codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Add column that replaces rate codes with string names\n",
    "\n",
    "rate_codes = {\n",
    "    1: 'Standard Rate',\n",
    "    2: 'JFK',\n",
    "    3: 'Newark',\n",
    "    4: 'Nassau or Westchester',\n",
    "    5: 'Negotiated fare',\n",
    "    6: 'Group Ride'\n",
    "}\n",
    "\n",
    "merged2013dffull['rate_code_name'] = merged2013dffull['rate_code'].map(lambda x: rate_codes[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Now let's plot the different Rate codes to see where people are going\n",
    "ratecode_tip = merged2013dffull.groupby(['rate_code_name'])['medallion'].count()\n",
    "ratecode_tip.head()\n",
    "ax = ratecode_tip.plot(kind='bar', x='rate_code_name', y='medallion', alpha=.6)\n",
    "ax.set_xlabel(\"Payment Type\")\n",
    "ax.set_ylabel(\"Number of Rides\")\n",
    "ax.grid(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our regression, we only care about standard rate fares, because they make up the vast majority of New York taxi trips, and because they are more correlated with tip amount & distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Only include standard rate in our regression\n",
    "merged2013dffull = merged2013dffull[merged2013dffull['rate_code'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some trip entries have distance =0, which means they are incomplete or never happened. Let's remove these so our entries all represent real trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Remove incomplete trips\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.trip_distance > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to remove payment types that don't help our data analysis. What payment methods do passengers use?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#First, let's get a list of all the unique payment types\n",
    "payment_types = list(merged2013dffull.payment_type.unique())\n",
    "payment_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The different payment methods are Cash, Disputed, No Charge, Credit Card, and Unknown. How many trips are made using each payment type? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate how many trips are made using each payment type\n",
    "payment_types = merged2013dffull.groupby(\"payment_type\")['medallion'].count()\n",
    "ax = payment_types.plot(kind=\"bar\")\n",
    "\n",
    "#add text values on top of the bars\n",
    "bars = ax.patches\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, height + 200, height, ha='center', va='bottom', fontsize=15)\n",
    "plt.savefig('images/payment_types')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We only care about analyzing Cash and Credit, so we can remove the others. Since the others make up < 1% of our dataset, removing them will not have a significant impact on our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Remove rare payment types\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.payment_type != 'UNK']\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.payment_type != 'DIS']\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.payment_type != 'NOC']\n",
    "merged2013dffull = merged2013dffull.reset_index(drop=True)\n",
    "print merged2013dffull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the  number of passengers paying cash vs. credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "payment_counts = merged2013dffull.groupby(['payment_type'])['medallion'].count()\n",
    "ax = payment_counts.plot(kind='bar', x='payment_type', y='medallion', alpha=.6)\n",
    "ax.set_xlabel(\"Payment Type\")\n",
    "ax.set_ylabel(\"Number of Rides\")\n",
    "ax.grid(False)\n",
    "ax.set_title('Number of Rides Paying Cash vs. Credit')\n",
    "payment_counts.head()\n",
    "plt.savefig('images/cash_vs_credit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "total_trips = float(merged2013dffull.shape[0])\n",
    "cash_trips = float(payment_counts['CSH']/total_trips*100)\n",
    "cc_trips = float(payment_counts['CRD']/total_trips*100)\n",
    "print \"%.2f\" % cash_trips, \"% of trips pay with cash\"\n",
    "print \"%.2f\" % cc_trips, \"% of trips pay with credit cards\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of riders who pay with credit card is slightly higher than the number of riders who pay with cash."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pickup and Dropoff Times\n",
    "Now we want to look at pickup and dropoff times. In order to do this, we need to convert our times to date objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert pickup and dropoff to date objects\n",
    "merged2013dffull['pickup_datetime'] = pd.to_datetime(merged2013dffull['pickup_datetime'])\n",
    "merged2013dffull['dropoff_datetime'] = pd.to_datetime(merged2013dffull['dropoff_datetime'])\n",
    "merged2013dffull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also add a column to convert our pickup times to number of seconds since midnight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def seconds_since_midnight(d):\n",
    "    ddelta = (d - d.replace(hour=0, minute=0, second=0, microsecond=0))\n",
    "    return ddelta.total_seconds()\n",
    "\n",
    "merged2013dffull['secs_since_midnight'] = merged2013dffull.pickup_datetime.apply(seconds_since_midnight)\n",
    "merged2013dffull = merged2013dffull[merged2013dffull['trip_time_in_secs'] > 0]\n",
    "merged2013dffull['hours_since_midnight'] = np.floor(merged2013df['secs_since_midnight']/3600).apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter out Cash transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a copy of cash transactions\n",
    "cashdf = merged2013dffull[merged2013dffull['payment_type'] == 'CSH']\n",
    "\n",
    "#We only want credit card transactions\n",
    "merged2013df = merged2013dffull[merged2013dffull['payment_type'] == 'CRD']\n",
    "merged2013df.reset_index(drop=True,inplace=True)\n",
    "print merged2013df.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume that outlier tips will skew our data, and are not correlated with other variables, so we can eliminate the unreasonable tips above 100%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Eliminate high tip outliers\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.tip_amount_normalized < 1.]\n",
    "merged2013dffull = merged2013dffull.reset_index(drop=True)\n",
    "print merged2013dffull.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to look at differences between vendors. According to the blog \"I Quant NYC\", taxi drivers can make more money depending on which vendor software they use. CMT vendor software includes tolls and surcharge as part of the fare, which is then used to calculate tip amount. VTS, however, does not, so VTS tips are generally lower. Let's examine this hypothesis. For passengers who did tip, did % tip differ among different vendor software?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Calculate average tip % for each vendor\n",
    "tips_above_zero = merged2013dffull[merged2013dffull.tip_amount > 0]\n",
    "vendor_tips= tips_above_zero.groupby(['vendor_id'])['tip_amount_normalized'].mean()\n",
    "ax = vendor_tips.plot(kind='bar', x='vendor_id', y='tip_amount_normalized')\n",
    "ax.set_xlabel(\"Vendor ID\")\n",
    "ax.set_ylabel(\"Average Tip %\")\n",
    "print vendor_tips\n",
    "plt.savefig('images/vendor_tips')\n",
    "\n",
    "# #add text values on top of the bars\n",
    "# bars = ax.patches\n",
    "\n",
    "# for bar in bars:\n",
    "#     height = bar.get_height()\n",
    "#     ax.text(bar.get_x() + bar.get_width()/2, height + 200, height, ha='center', va='bottom', fontsize=15)\n",
    "# plt.savefig('images/vendor_tips')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "While there is a 0.5% difference between the two vendors, the difference is not large. Let's look at the number of trips made by each vendor software. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculate number of trips made by each vendor\n",
    "vendor_fares = merged2013dffull.groupby(['vendor_id'])['medallion'].count()\n",
    "ax = vendor_fares.plot(kind='bar', x='vendor_id', y='medallion')\n",
    "ax.set_xlabel(\"Vendor ID\")\n",
    "ax.set_ylabel(\"# Trips per vendor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#How many trips use CMT vs. VTS?\n",
    "vendor_fares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less than .3% of trips use CMT software, so we can remove these trips and only consider VTS software for consistency in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remove CMT, because it calculates tipping data differently and may skew our results\n",
    "merged2013dffull = merged2013dffull[merged2013dffull.vendor_id == 'VTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# So what is the tip distribution like?\n",
    "merged2013df.tip_amount_normalized.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For those that tip, what percentage of the total fare cost do they tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "avg_tip_percent = merged2013df[merged2013df.tip_amount_normalized > 0].tip_amount_normalized.mean()\n",
    "avg_tip_percent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And what percentage of people actually tip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "float(len(merged2013df[merged2013df.tip_amount > 0])) / float(len(merged2013df.tip_amount))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2013df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##How much cash tip goes unreported?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate the baseline number of tips that goes unreported by calculating the average credit card tip and multiplying it by the total sum of cash fares. This method is a baseline, and has some flaws, which we will expand on in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cash_sums = cashdf['fare_amount'].sum()\n",
    "cash_sums*avg_tip_percent "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this subset of 20 days, there is an estimated $872,383 worth of tips that goes unreported. Imagine if we calculated unreported cash tips on the entire dataset!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at how different variables are correlated. First, we look at the total revenue per hour for the taxis, and how it changes throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fare_df = merged2013df.groupby(['hours_since_midnight'])['fare_amount'].sum()\n",
    "ax = fare_df.plot(kind='line', x='hours_since_midnight', y='fare_amount',color='c',title='Average Total Revenue per Hour')\n",
    "ax.set_xlabel(\"Hours since Midnight\")\n",
    "ax.set_ylabel(\"Revenue ($USD)\")\n",
    "ax.set_ylim(0,400000)\n",
    "\n",
    "plt.savefig('images/revenue_by_hour.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The taxi companies make the most money during rush hours, which occur between 7-8am and 5-6pm. They make the least money in the middle of the night. It turns out that even in the city that never sleeps, people still use fewer taxis in the middle of the night! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Rides per hour at different times of day\n",
    "rides_per_hour = merged2013df.groupby(['hours_since_midnight'])['medallion'].count()\n",
    "ax = rides_per_hour.plot(kind='line', x='hours_since_midnight', y='medallion')\n",
    "ax.set_xlabel(\"Hours since Midnight\")\n",
    "ax.set_ylabel(\"# Rides per hour\")\n",
    "plt.savefig(\"images/rides_per_hour_vs_timeofday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not surprisingly, the number of rides and total revenue are closely correlated. Now let's see if average tip % changes throughout the day too!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2013df['hours_since_midnight'] = np.floor(merged2013df['secs_since_midnight']/3600).apply(lambda x: int(x))\n",
    "small_df = merged2013df.groupby(['hours_since_midnight'])['tip_amount_normalized'].mean().apply(lambda x: x*100)\n",
    "ax = small_df.plot(kind='bar', x='hours_since_midnight', y='tip_amount_normalized', alpha=.4, title=\"Average Tip by Time of Day\"\n",
    "                   , color=['c','c','c','c','c','r','r','r','r','r','r','r','b','b','b','b','b'\\\n",
    "                            ,'b','b','c','c','c','c','c','c'])\n",
    "plt.ylim((17.5,21.5))\n",
    "ax.set_xlabel(\"Hours since Midnight\")\n",
    "ax.set_ylabel(\"Average Tip %\")\n",
    "plt.savefig(\"images/tip_by_hour.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the graph, the largest % tips, on average, occur at night between 4-7pm. The smallest tips occur in the morning between 6am-9am. Interestingly, people tip less in the morning, and more at night! Maybe people's psychological state changes throughout the day? This would be interesting to analyze in more depth. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Average fare per minute plotted against time of day\n",
    "merged2013df['cost_per_minute'] = merged2013df['fare_amount']/(merged2013df['trip_time_in_secs']/60)\n",
    "\n",
    "fare_per_minute = merged2013df.groupby(['hours_since_midnight'])['cost_per_minute'].mean()\n",
    "ax = fare_per_minute.plot(kind='line', x='hours_since_midnight', y='cost_per_minute')\n",
    "ax.set_ylim(0,1.4)\n",
    "ax.set_xlabel(\"Hours since Midnight\")\n",
    "ax.set_ylabel(\"Cost per Minute ($US)\")\n",
    "plt.savefig(\"images/cost_per_minute_vs_timeofday\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite having the fewest rides, lowest revenue, and smallest average tip percentage, the morning time has the highest cost per minute for a rider.\n",
    "\n",
    "Now let's look at a histogram of tipper tendencies. What is the most common range for tips?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#remove those with extremely high tips\n",
    "low_tips_df = merged2013df[merged2013df['tip_amount_normalized'] < .5]\n",
    "low_tips = low_tips_df['tip_amount_normalized']\n",
    "ax = low_tips.plot(kind='hist', alpha=0.5, bins=8)\n",
    "ax.set_xlim(0,0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we would expect, most passengers tip between 20%-25%. Let's dive into smaller bins to see if we can extract more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "low_tips = low_tips_df['tip_amount_normalized']\n",
    "low_tips.plot(kind='hist', alpha=0.5, bins=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breaking down the histogram into greater detail, we see peaks at 0%, 20%, 25% and 30%- the default tip options on the credit card readers in cabs! This means our baseline for predicting cash tips may be less accurate, because cash tips don't have these default tip options. The histogram for cash tips would likely be more evenly distributed, and the peaks would not be as high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Popular Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##Number of rides from different longitudes\n",
    "longitude_frequency = merged2013df.groupby(['pickup_longitude'])['medallion'].count()\n",
    "ax = longitude_frequency.plot(kind='area', x='pickup_longitude', y='medallion',colormap='cubehelix')\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"# Rides\")\n",
    "plt.savefig(\"images/rides_per_hour_vs_longitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longitude_frequency.sort(ascending=False,inplace=True)\n",
    "longitude_frequency.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Number of rides from different latitudes\n",
    "latitude_frequency = merged2013df.groupby(['pickup_latitude'])['medallion'].count()\n",
    "ax = latitude_frequency.plot(kind='area', x='pickup_latitude', y='medallion',colormap='cubehelix')\n",
    "ax.set_xlabel(\"Latitude\")\n",
    "ax.set_ylabel(\"# Rides\")\n",
    "plt.savefig(\"images/rides_per_hour_vs_latitude\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latitude_frequency.sort(ascending=False,inplace=True)\n",
    "latitude_frequency.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Part 1: Initial Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get a sense of the data we're working with. Using Matplotlib's Basemap, we can see the pickup locations for our data subset. *(Warning: this takes about 5 minutes to run)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Basemap(projection='merc',llcrnrlat=40.55,urcrnrlat=40.82,\\\n",
    "            llcrnrlon=-74.1, urcrnrlon=-73.82, lat_ts=40.5,resolution='f')\n",
    "m.drawmapboundary(fill_color='#85A6D9')\n",
    "m.drawcoastlines(color='#6D5F47', linewidth=.4)\n",
    "m.drawrivers(color='#6D5F47', linewidth=.4)\n",
    "m.fillcontinents(color='white', lake_color='#85A6D9')\n",
    "\n",
    "#m.scatter(merged2013df.pickup_longitude.values, merged2013df.pickup_latitude.values, alpha = 0.006, latlon=True, zorder=10)\n",
    "for i in range(300000):\n",
    "    m.plot(merged2013df.pickup_longitude[i], merged2013df.pickup_latitude[i],'ro',latlon=True,ms=1,alpha=.3)\n",
    "\n",
    "plt.savefig('images/pickup_init.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's create a visualization for a single cab driver to see their pickup and "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = Basemap(projection='merc',llcrnrlat=40.55,urcrnrlat=40.82,\\\n",
    "            llcrnrlon=-74.1, urcrnrlon=-73.82, lat_ts=40.5,resolution='h')\n",
    "m.drawmapboundary(fill_color='#85A6D9')\n",
    "m.drawcoastlines(color='#6D5F47', linewidth=.4)\n",
    "m.drawrivers(color='#6D5F47', linewidth=.4)\n",
    "m.fillcontinents(color='white', lake_color='#85A6D9')\n",
    "\n",
    "ex_cab = merged2013df[merged2013df.medallion == '000318C2E3E6381580E5C99910A60668'].reset_index(drop=True)\n",
    "mx_pickup, my_pickup = list(ex_cab.pickup_longitude),list(ex_cab.pickup_latitude)\n",
    "mx_dropoff, my_dropoff = list(ex_cab.dropoff_longitude),list(ex_cab.dropoff_latitude)\n",
    "assert len(mx_pickup) == len(my_pickup) == len(mx_dropoff) == len(my_dropoff)\n",
    "\n",
    "for i in range(len(mx_pickup)):\n",
    "    m.plot(mx_pickup[i], my_pickup[i],'ro',latlon=True,ms=3,alpha=1)\n",
    "    m.plot(mx_dropoff[i], my_dropoff[i],'go',latlon=True,ms=3,alpha=1)\n",
    "\n",
    "plt.title(\"Pickup and Dropoff Points for Cab #000318C2E3E6381580E5C99910A60668\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geocoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'd like to be able to convert our pairs of coordinates from the pickup and dropoff points into map-like information about the location - like an address. In order to do this, we take advantage of the Google Maps Geocoding API. Our use is limited by the rate query: 10 requests per second, 2500 free requests per 24 hours.\n",
    "\n",
    "In order to take advantage of it, go to the following site and register your IP address: https://developers.google.com/maps/documentation/geocoding/get-api-key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've registered, enter your API key here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "geocoding_api_key = 'AIzaSyCDqqN3Ky2lnba6p23VAKzIgrvsTwZwzM0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `rev_geocode` takes a latitude and longitude pair, and returns the JSON output from the Google Maps Geocoding API, which contains detailed geographical information about the coordinate pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "def rev_geocode(latitude, longitude):\n",
    "    API_str = 'https://maps.googleapis.com/maps/api/geocode/json?latlng=' \\\n",
    "    + str(latitude) + ',' + str(longitude) + '&key=' + geocoding_api_key\n",
    "    return requests.get(API_str).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev_geocode(-73.978165, 40.757977)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we create some histograms to examine the frequency of the important columns in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "hist_columns = ['passenger_count','trip_time_in_secs', 'trip_distance','fare_amount', \\\n",
    "                'tip_amount', 'total_amount', 'tip_amount_normalized', 'secs_since_midnight']\n",
    "fig, ax_lst = plt.subplots(4, 2)\n",
    "axes = itertools.chain.from_iterable(ax_lst)\n",
    "\n",
    "quantiles = merged2013df.quantile(.90)\n",
    "for column in hist_columns:\n",
    "    merged2013df = merged2013df[merged2013df[column] <= quantiles[column]]\n",
    "\n",
    "for column in hist_columns:\n",
    "    ax = next(axes)\n",
    "    ax.hist(merged2013df[column].values)\n",
    "    #ax.set_title(\"Histogram\")\n",
    "    ax.set_xlabel(column)\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.legend()\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: The (erratic) art of tipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `merged2013df` as our base dataset for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged2013df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by creating a dictionary called `tipclassifiers` that contains some basic information about the tipping habits of people. We figured out the amount of tip and percentage with respect to the total fare for all riders and riders that actually give a tip. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tipstats = {}\n",
    "tipstats['tip'] = merged2013df[merged2013df.tip_amount > 0]\n",
    "tipstats['no_tip'] = merged2013df[merged2013df.tip_amount == 0]\n",
    "tipstats['tip_perc'] = float(len(merged2013df[merged2013df.tip_amount > 0])) / float(len(merged2013df))\n",
    "tipstats['tip_mean'] = merged2013df.tip_amount.mean()\n",
    "tipstats['pos_tip_mean'] = tipstats['tip'].tip_amount.mean()\n",
    "tipstats['tip_norm_mean'] = merged2013df.tip_amount_normalized.mean()\n",
    "tipstats['pos_tip_norm_mean'] = tipstats['tip'].tip_amount_normalized.mean()\n",
    "print \"Percentage of riders that tip: %.3f%%\" % (tipstats['tip_perc'] * 100)\n",
    "print \"Tip amounts (all riders, riders that tip) $%.2f, $%.2f\" % (tipstats['tip_mean'], tipstats['pos_tip_mean'])\n",
    "print \"Tip percentages (all riders, riders that tip): %.3f%%, %.3f%%\" % (tipstats['tip_norm_mean'] * 100, tipstats['pos_tip_norm_mean'] * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Tipping based on fare amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting question we can ask is whether we can predict whether a rider will tip on a cab trip. Intuitively, we might imagine that people tend to tip based on a variety of traits. Perhaps a banker on Wall Street or someone with a relatively high income is more likely to be generous with a tip. It's plausible that riders at night are more likely to tip because they are out for entertainment or on dates, or maybe more daytime riders tip because many professionals bill the taxi fare to the company they work rather than their own credit and are therefore more likely to be generous.\n",
    "\n",
    "In this section we'll examine what correlations exist (if any) between a rider's decision to tip and other factors related to the cab ride.\n",
    "\n",
    "First, we'll create a regression that examines whether fare amount is correlated with the decision to tip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.formula.api import ols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Logistic regression** is a probabilistic model that links observed binary data to a set of features.\n",
    "\n",
    "Suppose that we have a set of binary (that is, taking the values 0 or 1) observations $Y_1,\\cdots,Y_n$, and for each observation $Y_i$ we have a vector of features $X_i$. The logistic regression model assumes that there is some set of **weights**, **coefficients**, or **parameters** $\\Theta$, one for each feature, so that the data were generated by flipping a weighted coin whose probability of giving a 1 is given by the following equation:\n",
    "\n",
    "$$\n",
    "P(Y_i = 1) = \\sigma(\\sum \\Theta_i X_i),\n",
    "$$\n",
    "\n",
    "where $\\sigma$ is the *sigmoid* (or logit) function\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{e^x}{1+e^x}.\n",
    "$$\n",
    "\n",
    "When we *fit* a logistic regression model, we determine values for each $\\Theta_i$ that allows the model to best fit the *training data* we have observed. Once we do this, we can use these coefficients to make predictions about data we have not yet observed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So let's create our known observations outcomes - that is, our X and y for the fare amount and whether the riders chose to tip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# X - fare amount\n",
    "X = merged2013df.fare_amount.values.reshape(len(merged2013df), 1)\n",
    "\n",
    "# y - decision to tip (binary feature)\n",
    "y = merged2013df.tip_amount.apply(lambda x: 1 if x > 0 else 0).values.ravel()\n",
    "\n",
    "# create a dataframe\n",
    "logit_data = pd.DataFrame(zip(X,y))\n",
    "logit_data.columns = ['fare', 'tipped']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and run logistic regression on the entire data set, and see how accurate it is!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# instantiate a logistic regression model, and fit with X and y\n",
    "logit_model = LogisticRegression()\n",
    "logit_model.fit(X, y)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "logit_model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "54% accuracy doesn't seem great, but what's the null error rate? That is, if we predicted a rider won't tip every time, how often would we be wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# y.mean() is the percentage that tip\n",
    "y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About 49% of riders gave no tip, which means that you could obtain 51% accuracy by always predicting \"yes\". So we're doing better than the positive error rate, but not in a meaningful way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly plot the fares against the decision to tip to visualize the findings from the our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlim([0, 300])\n",
    "plt.ylim([-0.1,1.1])\n",
    "plt.title(\"Fare amount vs. Decision to tip\")\n",
    "plt.xlabel(\"Fare ($)\")\n",
    "plt.ylabel(\"Tipped?\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the regression score suggested and the visualization demonstrates, the correlation is virtually non-existent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Finding other correlations with tip amount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also consider other potential factors that influence how much riders tip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = ols('tip_amount ~ fare_amount + passenger_count + trip_time_in_secs + payment_idx + pickup_latitude + pickup_longitude + secs_since_midnight', merged2013df).fit()\n",
    "print m.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use $R^2$ as a measure of how well data fits our linear model. It represents a ratio of the explained variance to the total variance. A value of 1 means the data fits perfectly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.rsquared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An $R^2$ value of $0.57$ and it implies that 57% of the variability between the two variables have been accounted for and the remaining 43% of the variability is still unaccounted for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Predicting Unreported Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous parts of this ipython notebook, we found that almost nobody who paid in cash had their tips recorded. This is likely because taxi drivers don't want to lose money to taxes that go along with reporting tips. We'd like to find out just how much money has gone unreported in our dataset.\n",
    "\n",
    "Above we created a very rough baseline by taking the average tip reported for customers who paid with credit, and multiplied it by the total amount spent by customers who paid with cash in the entire dataset. The assumption behind this baseline is that people who pay in cash don't tip fundamentally differently from those who pay with credit. This seems like a fair assumption to make. It's highly unlikely that people who pay cash simply don't tip.\n",
    "\n",
    "We'll employ a more sophisticated method to try to predict unreported tips using kNN classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN Classification Applied to tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN is a classification method that is used to predict to which class a certain data point belongs. It makes sense to think about tips in terms of classes because we think about tips fundamentally in terms of classes anyway. For example, we may think that a 10-15% tip is a reasonable tip (perhaps a bit on the low side), whereas a tip that is between 15-25% is a very good tip. Anything above that is seen as a very large tip.\n",
    "\n",
    "The first step in this process is to create a classifier that identifies to which class a tip belongs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "# make a copy of the df to use in machine learning classification\n",
    "# credit df and cash df\n",
    "cash_df = merged2013df[merged2013df.payment_type == 'CSH']\n",
    "crd_df = merged2013df[merged2013df.payment_type == 'CRD']\n",
    "\n",
    "# remove outlier tips above 40%\n",
    "small_crd_df = crd_df[crd_df.tip_amount_normalized <= .4]\n",
    "\n",
    "# classify tips into buckets based on 2% intervals\n",
    "small_crd_df['tip_class'] = small_crd_df.tip_amount_normalized.apply(lambda x: math.floor(x*100/2.0))\n",
    "\n",
    "cols = ['passenger_count','trip_time_in_secs','trip_distance','fare_amount','secs_since_midnight','pickup_latitude','pickup_longitude']\n",
    "\n",
    "# remove outliers from classifiers for plotting\n",
    "small_crd_df_plot = small_crd_df[small_crd_df['trip_time_in_secs'] <= 4000]\n",
    "small_crd_df_plot = small_crd_df_plot[small_crd_df_plot['trip_distance'] <= 25]\n",
    "small_crd_df_plot = small_crd_df_plot[small_crd_df_plot['fare_amount'] <= 75]\n",
    "small_crd_df_plot = small_crd_df_plot[small_crd_df_plot['pickup_longitude'] <= -73.5]\n",
    "small_crd_df_plot = small_crd_df_plot[small_crd_df_plot['pickup_longitude'] >= -74.2]\n",
    "small_crd_df_plot = small_crd_df_plot[small_crd_df_plot['pickup_latitude'] <= 41]\n",
    "small_crd_df_plot = small_crd_df_plot[small_crd_df_plot['pickup_latitude'] >= 40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_crd_df[['tip_amount_normalized','tip_class']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tips are now sorted into buckets on a 5% interval. This will hopefully allow us to predict which bucket cash tips would have fallen into had they been recorded based on the other features of the ride. Essentially, we'll find out which rides that were paid for in credit are closest to any given cash ride point (in Euclidean space) and use those to classify the credit point. This requires us the split the data into training and test sets.\n",
    "\n",
    "Before we actually get into doing the kNN classifications, let's examine some of the different classifiers we could use. Ideally we'd like to used classifiers which are clear differentiators between tipping buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "small_crd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create the subplots figure\n",
    "fig, ax = plt.subplots(nrows=4, ncols=2, figsize=(30,50), tight_layout=True)\n",
    "\n",
    "for a, c in zip(ax.ravel(), cols):\n",
    "\n",
    "    # different tip classes\n",
    "    tc_0 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 0][c]\n",
    "    tc_1 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 1][c]\n",
    "    tc_2 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 2][c]\n",
    "    tc_3 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 3][c]\n",
    "    tc_4 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 4][c]\n",
    "    tc_5 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 5][c]\n",
    "    tc_6 = small_crd_df_plot.loc[small_crd_df_plot['tip_class'] == 6][c]\n",
    "\n",
    "    with sns.color_palette('muted'):\n",
    "    # plot data\n",
    "        sns.kdeplot(tc_0, color='blue', label='Tip Class 0', ax=a, shade=True)\n",
    "        sns.kdeplot(tc_1, color='red', label='Tip Class 1', ax=a, shade=True)\n",
    "        sns.kdeplot(tc_2, color='green', label='Tip Class 2', ax=a, shade=True)\n",
    "        sns.kdeplot(tc_3, color='yellow', label='Tip Class 3', ax=a, shade=True)\n",
    "        sns.kdeplot(tc_4, color='purple', label='Tip Class 4', ax=a, shade=True)\n",
    "        sns.kdeplot(tc_5, color='pink', label='Tip Class 5', ax=a, shade=True)\n",
    "        sns.kdeplot(tc_6, color='brown', label='Tip Class 6', ax=a, shade=True)\n",
    "\n",
    "    # plot semantics\n",
    "    a.set_title(c.replace('_',' ').capitalize())\n",
    "    a.set_xlabel('Value')\n",
    "    a.set_ylabel('Density')\n",
    "    a.grid(False)\n",
    "\n",
    "fig.savefig('images/KDE_kNN.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A first glance tells us that none of these classifiers will be great for differentiating between tip classes, since many of them are very similar. It looks as though we'll be able to classify some outlier points relativelly well based on a single classifier, but the vast majority of point will most likely be hard to predict based on the data that we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kNN Classification and Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(merged2013df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_train_X = train_df[['trip_time_in_secs', 'trip_distance', 'fare_amount']]\n",
    "knn_train_y = train_df.tip_class\n",
    "knn_test_X = test_df[['trip_time_in_secs', 'trip_distance', 'fare_amount']]\n",
    "knn_test_y = test_df.tip_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Classifier\n",
    "knn_clf = KNeighborsClassifier()\n",
    "knn_params = dict(n_neighbors=[10,50,100], weights=['uniform','distance'])\n",
    "knn_grid_search = GridSearchCV(knn_clf, param_grid=knn_params)\n",
    "knn_grid_search.fit(knn_train_X, knn_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Regressor\n",
    "knn_reg = KNeighborsRegressor()\n",
    "knn_params = dict(n_neighbors=[10,50,100], weights=['uniform','distance'])\n",
    "knn_reg_grid_search = GridSearchCV(knn_reg, param_grid=knn_params)\n",
    "knn_reg_grid_search.fit(knn_train_X, knn_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_loc_train_X = train_df[['pickup_latitude', 'pickup_longitude']]\n",
    "knn_loc_train_y = train_df.tip_class\n",
    "knn_loc_test_X = test_df[['pickup_latitude', 'pickup_longitude']]\n",
    "knn_loc_test_Y = test_df.tip_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# classifier on location\n",
    "knn_loc_clf = KNeighborsClassifier()\n",
    "knn_loc_grid_search = GridSearchCV(knn_loc_clf, param_grid=knn_params)\n",
    "knn_loc_grid_search.fit(knn_loc_train_X, knn_loc_train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['passenger_count',\n",
    "            'trip_time_in_secs',\n",
    "            'trip_distance',\n",
    "            'pickup_longitude',\n",
    "            'pickup_latitude',\n",
    "            'dropoff_longitude',\n",
    "            'dropoff_latitude',\n",
    "            'fare_amount',\n",
    "            'surcharge',\n",
    "            'mta_tax',\n",
    "            'tolls_amount',\n",
    "            'secs_since_midnight',\n",
    "            'hours_since_midnight']\n",
    "nn_train_X = train_df[features]\n",
    "nn_train_y = train_df.tip_class\n",
    "nn_test_X = test_df[features]\n",
    "nn_test_y = test_df.tip_class\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(nn_train_X)\n",
    "nn_train_X = scaler.transform(nn_train_X)\n",
    "nn_test_X = scaler.transform(nn_test_X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# warning, takes over an hour to run\n",
    "nn_clf = MLPClassifier(tol = 1e-3)\n",
    "nn_params = dict(alpha=10.0 ** -np.arange(1, 7), hidden_layer_sizes=np.array([(75,), (50,), (40,25)]))\n",
    "nn_grid_search = GridSearchCV(nn_clf, param_grid=nn_params)\n",
    "nn_grid_search.fit(nn_train_X, nn_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print knn_grid_search.best_score_\n",
    "print knn_reg_grid_search.best_score_\n",
    "print knn_loc_grid_search.best_score_\n",
    "print lr_grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def class_to_tip_percent(tip_class):\n",
    "    return max(tip_class * 5.0 / 100 - .025,0)\n",
    "\n",
    "to_tip_perc = np.vectorize(lambda x: class_to_tip_percent(x))\n",
    "\n",
    "baseline_predictions = np.ones_like(test_df.tip_class.values) * sp.stats.mode(test_df.tip_class.values)[0][0]\n",
    "baseline_sum = (to_tip_perc(baseline_predictions) * test_df.fare_amount).sum()\n",
    "\n",
    "nn_predictions = nn_grid_search.best_estimator_.predict(nn_test_X)\n",
    "nn_sum = (to_tip_perc(nn_predictions) * test_df.fare_amount).sum()\n",
    "\n",
    "knn_predictions = knn_grid_search.best_estimator_.predict(knn_test_X)\n",
    "knn_sum = (to_tip_perc(knn_predictions) * test_df.fare_amount).sum()\n",
    "\n",
    "knn_loc_predictions = knn_loc_grid_search.best_estimator_.predict(knn_loc_test_X)\n",
    "knn_loc_sum = (to_tip_perc(knn_loc_predictions) * test_df.fare_amount).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tip_sum = test_df.tip_amount.sum()\n",
    "total = float(test_df.tip_class.count())\n",
    "print \"Ground truth: \", tip_sum\n",
    "print \"Baseline prediction: \", baseline_sum, \"| Percent Error: \", (baseline_sum - tip_sum)/tip_sum\n",
    "print \"kNN prediction: \", knn_sum, \"| Percent Error: \", (knn_sum - tip_sum)/tip_sum\n",
    "print \"kNN location prediction: \", knn_loc_sum, \"| Percent Error: \", (knn_loc_sum - tip_sum)/tip_sum\n",
    "print \"NN prediction: \", nn_sum, \"| Percent Error: \", (nn_sum - tip_sum)/tip_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cash_df = merged2013dffull[merged2013dffull.payment_type == 'CSH']\n",
    "cash_input = cash_df[features]\n",
    "cash_predictions = nn_grid_search.best_estimator_.predict(cash_input)\n",
    "(func(cash_predictions) * cash_df.fare_amount.values).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we have no ground truth to check against, that is our best guess for the amount of tip that went unreported from just over a million rides. With some more computing power and a larger dataset, one could guess how much is unreported in a month, year, or any interval we'd like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Other Visualizations & Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medallion_tips = merged2013df.groupby(['medallion'])['tip_amount_normalized'].mean()\n",
    "medallion_tips.plot(kind='hist', alpha=0.5, bins=21)\n",
    "plt.xlim(0,.4)\n",
    "ax.set_xlabel(\"Avg. tip for a medallion\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "print \"Maximum Avg. tip for a Medalion = \", medallion_tips.max()\n",
    "print \"Minimum Avg. tip for a Medalion = \", medallion_tips.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medallion_revenue = merged2013df.groupby(['medallion'])['fare_amount'].sum()\n",
    "ax = medallion_revenue.plot(kind='hist', alpha=0.5, bins=21)\n",
    "#plt.xlim(0,.4)\n",
    "ax.set_xlabel(\"Revenue per Medallion\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "#print \"Maximum Avg. tip for a Medalion = \", medallion_tips.max()\n",
    "#print \"Minimum Avg. tip for a Medalion = \", medallion_tips.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "medallion_revenue = merged2013df.groupby(['medallion'])['fare_amount'].mean()\n",
    "medallion_revenue.plot(kind='hist', alpha=0.5, bins=50)\n",
    "ax.set_xlabel(\"Avg. fare for a medallion\")\n",
    "ax.set_ylabel(\"Frequency\") \n",
    "#plt.xlim(0,75)\n",
    "#print \"Maximum Avg. tip for a Medalion = \", medallion_tips.max()\n",
    "#print \"Minimum Avg. tip for a Medalion = \", medallion_tips.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vendor_fares = merged2013df.groupby(['vendor_id'])['fare_amount'].sum()\n",
    "#merged2013df.groupby(['hours_since_midnight'])['tip_amount_normalized'].mean()\n",
    "#merged2013df.groupby('hours_since_midnight')['tip_amount_normalized'].head()\n",
    "ax = vendor_fares.plot(kind='bar', x='vendor_id', y='fare_amount')\n",
    "ax.set_xlabel(\"Vendor ID\")\n",
    "ax.set_ylabel(\"Revenue ($USD)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "longitude_tip = merged2013df.groupby(['pickup_longitude'])['tip_amount_normalized'].mean()\n",
    "longitude_tip.plot(kind='hist', alpha=0.5, bins=50)\n",
    "ax.set_xlabel(\"Avg. fare for a medallion\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "#plt.xlim(0,.5)\n",
    "#print \"Maximum Avg. tip for a Medalion = \", medallion_tips.max()\n",
    "#print \"Minimum Avg. tip for a Medalion = \", medallion_tips.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "latitude_tip = merged2013df.groupby(['pickup_latitude'])['tip_amount_normalized'].mean()\n",
    "latitude_tip.plot(kind='hist', alpha=0.5, bins=50)\n",
    "ax.set_xlabel(\"Latitude of pickup\")\n",
    "ax.set_ylabel(\"Frequency\")\n",
    "#plt.xlim(0,.5)\n",
    "#print \"Maximum Avg. tip for a Medalion = \", medallion_tips.max()\n",
    "#print \"Minimum Avg. tip for a Medalion = \", medallion_tips.min()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
